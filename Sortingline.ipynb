{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This Workshop is created to give an introduction into the building of Convolutional Neural Network with the help of Tensorflow. \\\n",
    "Please check the Requirements in the Readme, before proceeding forward.\n",
    "## Goal\n",
    "The Goal is to provide a step by step solution to build a model, which seperates good an bad parts on a end-of-line model\n",
    "\n",
    "![Alt Text](pictures/foto_Modell.png)\n",
    "\n",
    "## steps\n",
    "The workshop will include multiple steps to create a model\n",
    "- inspecting the trainingsdata\n",
    "- load the data and label it\n",
    "- build a simple model\n",
    "- improve the model\n",
    "- test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "To work create a model and work with it a few imports are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#helpful ml modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "#Tensorflow and keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#create sequential models\n",
    "from tensorflow.keras.models import Sequential\n",
    "#import the layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, InputLayer, Dropout\n",
    "#datagenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "#augmentation\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) inspect the datasets\n",
    "There are two trainingsets in this project, one with 4 subfolders and one with 6 subfolders.\n",
    "- Which assumptions can you make for the model architecture with this information?\n",
    "- Load the shape of an image using the cv2 module\n",
    "- which information can you take away from the shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320, 3)\n"
     ]
    }
   ],
   "source": [
    "#load the image image = cv2...\n",
    "image = \n",
    "#print the image shape\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) creating datasets\n",
    "To use the given datasets to train a network they need to be loaded. You can do this using cv2 or the keras ImageDataGenrator. Create one Trainings and dataset using both solutions. Decide if you want 4 or 6 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) create a dataset using cv2 and for loops\n",
    "- how many lists do you need?\n",
    "- how can you get to the images?\n",
    "- What functions do you need from cv2\n",
    "- how are you going to label your data? (hint: look at the files or folders)\n",
    "- how will you normalize the data?\n",
    "- print the number of images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 840\n",
      "Number of training labels: 840\n"
     ]
    }
   ],
   "source": [
    "# trainingsset\n",
    "# Initialize empty lists for data and labels\n",
    "\n",
    "# Define the path to your training dataset directory\n",
    "training_directory = \n",
    "\n",
    "# Iterate over subdirectories (classes) in the training dataset directory\n",
    "for class_name in os.listdir(training_directory):\n",
    "    class_dir = os.path.join(training_directory, class_name)\n",
    "\n",
    "    # Iterate over image files in the class directory\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "        # Load image\n",
    "\n",
    "        # Normalize pixel values\n",
    "\n",
    "        # Append image to data list\n",
    "\n",
    "\n",
    "        # Assign numerical label based on filename.  Here is a if statement needed\n",
    "\n",
    "\n",
    "# Print the loaded data and labels\n",
    "print(\"Number of training samples:\", len(x_train))\n",
    "print(\"Number of training labels:\", len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected:\\\n",
    "Number of training samples: 840\\\n",
    "Number of training labels: 840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 360\n",
      "Number of test labels: 360\n"
     ]
    }
   ],
   "source": [
    "# testset\n",
    "# Initialize empty lists for data and labels\n",
    "\n",
    "# Define the path to your training dataset directory\n",
    "training_directory = \n",
    "\n",
    "# Iterate over subdirectories (classes) in the training dataset directory\n",
    "for class_name in os.listdir(training_directory):\n",
    "    class_dir = os.path.join(training_directory, class_name)\n",
    "\n",
    "    # Iterate over image files in the class directory\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "        # Load image\n",
    "\n",
    "        # Normalize pixel values\n",
    "\n",
    "        # Append image to data list\n",
    "\n",
    "        # Assign numerical label based on filename. Here is a if statement needed\n",
    "\n",
    "\n",
    "# Print the loaded data and labels\n",
    "print(\"Number of test samples:\", len(x_test))\n",
    "print(\"Number of test labels:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected:\\\n",
    "Number of training samples: 360\\\n",
    "Number of training labels: 360\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) create a dataset using ImageDataGenerator\n",
    "- What functions do you need from ImageDatagenerator\n",
    "- how are you going to label your data? (hint: look at the files or folders)\n",
    "- how will you normalize the data?\n",
    "- which configurations can be enabled?\n",
    "- print the class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 840 images belonging to 6 classes.\n",
      "Class indices: {'blue': 0, 'blue_fail': 1, 'red': 2, 'red_fail': 3, 'white': 4, 'white_fail': 5}\n"
     ]
    }
   ],
   "source": [
    "#trainingset\n",
    "# traindirectory\n",
    "data_dir_train = ''\n",
    "\n",
    "# generate Datengeneratoren\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "# Training Datengenerator\n",
    "train_generator = datagen.\n",
    "\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Print the class indices\n",
    "print(\"Class indices:\", class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected:\\\n",
    "Found 840 images belonging to 4 classes.\n",
    "Class indices: {'blue': 0, 'fail': 1, 'red': 2, 'white': 3}\\\n",
    "OR\\\n",
    "Found 840 images belonging to 6 classes.\n",
    "Class indices: {'blue': 0, 'blue_fail': 1, 'red': 2, 'red_fail': 3, 'white': 4, 'white_fail': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 360 images belonging to 6 classes.\n",
      "Class indices: {'blue': 0, 'blue_fail': 1, 'red': 2, 'red_fail': 3, 'white': 4, 'white_fail': 5}\n"
     ]
    }
   ],
   "source": [
    "# Testset\n",
    "# testdirectory\n",
    "data_dir_eval = ''\n",
    "\n",
    "# configure evaluationgenerator\n",
    "eval_datagen = ImageDataGenerator()\n",
    "\n",
    "# create a evaluationset\n",
    "eval_generator = eval_datagen.\n",
    ")\n",
    "class_indices = eval_generator.class_indices\n",
    "\n",
    "# Print the class indices\n",
    "print(\"Class indices:\", class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected:\\\n",
    "Found 360 images belonging to 4 classes.\n",
    "Class indices: {'blue': 0, 'fail': 1, 'red': 2, 'white': 3}\\\n",
    "OR\\\n",
    "Found 360 images belonging to 6 classes.\n",
    "Class indices: {'blue': 0, 'blue_fail': 1, 'red': 2, 'red_fail': 3, 'white': 4, 'white_fail': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a Classification model\n",
    "Start with a classification model. This should be able to predict between the Classes you chose (4 or 6). Start to utilize the imported layers of Keras.\n",
    "- What is the Input dimension?\n",
    "- Where do you configure the Input?\n",
    "- How is the last Layer configured?\n",
    "- Which metric and lossfuntion do you choose?\n",
    "- where is the flatten layer needed?\n",
    "- start with the sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start to create a Basic sequential model\n",
    "model = Sequential()\n",
    "#start adding layers here\n",
    "\n",
    "#compile your model\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt, loss='', metrics=[''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a) train the model\n",
    "- option1: use the two lists generated with the cv2 workflow (think about the batch size)\n",
    "- option2: use the datagen sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit('''put your traingsset here''', epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b)evaluate the model\n",
    "Evaluate your model, again there are two options.\n",
    "- How is the accuracy?\n",
    "- How can it be optimized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate('''put your traingsset here''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Choose optimization methods if needed \n",
    "Underfitting (your model cant get a good trainings accuracy)?:\n",
    "\n",
    "- Increase the amount of epochs\n",
    "- Change the Architecture (More Dense Layers, more Conv2d Layers...)\n",
    "\n",
    "### \n",
    "Overfitting (your model has a good training accuracy, bad a bad validation accuracy)\n",
    "- Research Dropoutlayers and add them to your model\n",
    "- research Keras Callback and early stopping\n",
    "- Research dataaugmentation and add it to either your model or in the dataset (ImageDataGenerator needs to be reconfigured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Export the model to .tflite\n",
    "if your model has achieved a good validation accuracy you can export it to a tflite model. This can then be loaded to the controller of the conveyor belt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Speichere das tflite-Modell\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) load the model on the Controller\n",
    "- open the RoboCoding Pro App\n",
    "- choose the option \"PROJEKT IMPORTIEREN\"\n",
    "- import the projekt \"sortingline_workshop.ft\" in the folder \"Fischertechnik_Projekt\"\n",
    "- now there should be an overview with the projekt \n",
    "- connect the Controller to the Computer with USB, for help view the file \"RoboProCodingTXT40Controller.pdf\" in th folder \"Fischertechnik_documentation\"\n",
    "- press this button in the top right corner: \\\n",
    "![Alt Text](pictures/Verbindung.png)\n",
    "- check the ip apperaing by choosing connect via usb\\\n",
    "![Alt Text](pictures/IP.png)\n",
    "- enter the IP in the Browser\n",
    "- login to the now opened webserver\n",
    "- Username: ft Password: fischertechnik\n",
    "- copy your tflite model to the folder custom/\n",
    "- change line 19 in the process.py file of the RoboCoding project to guide to your model on the webserver\n",
    "- load the programm on the Controller, by pressing the button left to the connect button\n",
    "- start the programm with the play button\n",
    "- error messages will be shown in the cconsol of the RoboCoding Pro App\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) A new Approach\n",
    "After checking the first model it is time for a new approach. \\\n",
    "The goal is to create a model that validates the color of a piece and a probability score if it is a good part.\\\n",
    "The following steps need to be redone:\n",
    "1. A new dataset needs to be created, use the cv2 method a think how it needs to be adapted. (Maybe a third list?)\n",
    "2. create a new model architecture, this time its a functional model, not a sequential.\n",
    "###\n",
    "Help will be given in sequential.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7a) Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset with cv2 and loops think about the changes.\n",
    "\n",
    "# trainingsset\n",
    "# Initialize empty lists for data and labels\n",
    "\n",
    "# Define the path to your training dataset directory\n",
    "training_directory = \n",
    "\n",
    "# Iterate over subdirectories (classes) in the training dataset directory\n",
    "for class_name in os.listdir(training_directory):\n",
    "    class_dir = os.path.join(training_directory, class_name)\n",
    "\n",
    "    # Iterate over image files in the class directory\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "        # Load image\n",
    "\n",
    "        # Normalize pixel values\n",
    "\n",
    "        # Append image to data list\n",
    "\n",
    "\n",
    "        # Assign numerical label based on filename.  Here is a if statement needed\n",
    "\n",
    "\n",
    "# Print the loaded data and labels\n",
    "print(\"Number of training samples:\", len(x_train))\n",
    "print(\"Number of training labels:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataset with cv2 and loops think about the changes.\n",
    "\n",
    "# testset\n",
    "# Initialize empty lists for data and labels\n",
    "\n",
    "# Define the path to your training dataset directory\n",
    "training_directory = \n",
    "\n",
    "# Iterate over subdirectories (classes) in the training dataset directory\n",
    "for class_name in os.listdir(training_directory):\n",
    "    class_dir = os.path.join(training_directory, class_name)\n",
    "\n",
    "    # Iterate over image files in the class directory\n",
    "    for image_name in os.listdir(class_dir):\n",
    "        image_path = os.path.join(class_dir, image_name)\n",
    "\n",
    "        # Load image\n",
    "\n",
    "        # Normalize pixel values\n",
    "\n",
    "        # Append image to data list\n",
    "\n",
    "        # Assign numerical label based on filename. Here is a if statement needed\n",
    "\n",
    "\n",
    "# Print the loaded data and labels\n",
    "print(\"Number of test samples:\", len(x_test))\n",
    "print(\"Number of test labels:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7b functional model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "\n",
    "# Convolutional Layer parameters\n",
    "num_filters = 100\n",
    "filter_size = (3,3)\n",
    "stride = 1\n",
    "\n",
    "# Pooling parameter\n",
    "pool_size = (2,2)\n",
    "# Density Layer parameters\n",
    "num_dense_units = 100\n",
    "\n",
    "# Additional parameters\n",
    "brightnessfactor = 0,25\n",
    "dropout_rate = 0,1\n",
    "shape = (240, 320, 3)\n",
    "num_classes = 3\n",
    "\n",
    "# Define input layer\n",
    "input_layer = Input(shape=shape)\n",
    "x = input_layer\n",
    "\n",
    "# Augmentation Layer\n",
    "\n",
    "\n",
    "# Convolutional layers\n",
    "\n",
    "\n",
    "# Output layers\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=[class_output, score_output])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compile youre model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your model\n",
    "# Extra parameter because there are 2 outputs with separate losses\n",
    "loss_weights = {'class_output': 0.175, 'score_output': 0.825}\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                      loss= {\n",
    "                            'class_output': tf.keras.losses.CategoricalCrossentropy(),\n",
    "                            'score_output': tf.keras.losses.BinaryCrossentropy()\n",
    "                            },\n",
    "                      metrics=['accuracy'],\n",
    "                      loss_weights = loss_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train\n",
    "remember the datset has changed, so the parameters in the fit function shoul be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### export to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Speichere das tflite-Modell\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finally repeat number 6 and test it in action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
